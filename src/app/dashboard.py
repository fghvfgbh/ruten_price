# src/app/dashboard.py (é›²ç«¯éƒ¨ç½²æœ€çµ‚ç‰ˆ)

import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime
import logging
from sqlalchemy import create_engine 

# ç¢ºä¿å°å…¥è·¯å¾‘æ­£ç¢º
from src.database.__init__ import get_db, init_db, engine 
from src.database.crud import create_or_update_product, add_price_record
from src.scraper.core import setup_driver, scrape_search_page 
from config import INITIAL_TRACKING_KEYWORDS, MAX_PAGES_TO_SCRAPE 
# --------------------------------------------------------

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- é›²ç«¯æ‰‹å‹•å•Ÿå‹•çˆ¬èŸ²å‡½å¼ ---
def run_scraper_manually():
    """åœ¨ Streamlit Cloud ç’°å¢ƒä¸­æ‰‹å‹•å•Ÿå‹•çˆ¬èŸ²ä»»å‹™ã€‚"""
    
    with st.status("æ­£åœ¨é‹è¡Œçˆ¬èŸ²...", expanded=True) as status:
        
        init_db() # ç¢ºä¿è³‡æ–™åº«çµæ§‹å­˜åœ¨
        
        driver = None
        db = None
        total_items_scraped = 0
        
        try:
            status.update(label="1/3 æ­£åœ¨å•Ÿå‹• WebDriver (Chrome)", state="running", expanded=True)
            driver = setup_driver()
            if not driver:
                status.update(label="WebDriver å•Ÿå‹•å¤±æ•—ï¼è«‹æª¢æŸ¥ Cloud æ—¥èªŒã€‚", state="error")
                return

            db_generator = get_db()
            db = next(db_generator)
            
            # 2. è¿­ä»£æ‰€æœ‰é—œéµå­—ä¸¦çˆ¬å–æ•¸æ“š
            for term in INITIAL_TRACKING_KEYWORDS:
                for page in range(1, MAX_PAGES_TO_SCRAPE + 1):
                    status.update(label=f"2/3 æ­£åœ¨çˆ¬å–é—œéµå­—: {term} (é ç¢¼: {page})...", state="running")
                    scraped_items = scrape_search_page(driver, term, page=page)
                    
                    if not scraped_items and page > 1: break
                    
                    for item in scraped_items:
                        product = create_or_update_product(db, **item)
                        if product:
                            add_price_record(db, product.id, item['price'])
                            total_items_scraped += 1
            
            # 3. æäº¤ä¸¦å®Œæˆ
            db.commit()
            status.update(label=f"âœ… æ•¸æ“šæ›´æ–°å®Œæˆï¼å…±è¨ˆæ–°å¢ {total_items_scraped} æ¢åƒ¹æ ¼è¨˜éŒ„ã€‚", state="complete")
            
        except Exception as e:
            if db: db.rollback()
            status.update(label=f"âŒ çˆ¬èŸ²ä»»å‹™åŸ·è¡ŒæœŸé–“ç™¼ç”ŸéŒ¯èª¤ï¼š{e}", state="error")
            logging.error(f"Cloud Scraper Error: {e}")
        finally:
            if driver: driver.quit()
            st.cache_data.clear() 
            st.rerun() 

# --- (å…¶é¤˜æ•¸æ“šè¼‰å…¥å’Œåœ–è¡¨å‡½å¼ä¿æŒä¸è®Š) ---
# ...
# --- Streamlit æ‡‰ç”¨ä¸»é«” ---
def main():
    st.set_page_config(layout="wide", page_title="éœ²å¤©åƒ¹æ ¼è¿½è¹¤å„€è¡¨æ¿")
    st.title("ğŸ’° éœ²å¤©æ‹è³£åƒ¹æ ¼è¶¨å‹¢è¿½è¹¤å„€è¡¨æ¿")
    st.markdown("---")

    init_db()
    
    # çˆ¬èŸ²å•Ÿå‹•æŒ‰éˆ•
    if st.button("æ‰‹å‹•æ›´æ–°æ•¸æ“š (é‹è¡Œçˆ¬èŸ²)"):
        run_scraper_manually()

    st.markdown("---")
    
    # ... (Tab 1 å’Œ Tab 2 é‚è¼¯ä¿æŒä¸è®Š) ...

if __name__ == '__main__':
    main()
